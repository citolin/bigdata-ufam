{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.window import Window\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.types import *\n",
    "%matplotlib inline\n",
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf().setAppName(\"Altigran P3\") \\\n",
    "                    .setMaster('local') \\\n",
    "                    .set('spark.executor.memory','14G') \\\n",
    "                    .set('spark.cores.max', '24') \\\n",
    "                    .set('spark.sql.tungsten.enabled', 'true')\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "sc._jsc.hadoopConfiguration().set('textinputformat.record.delimiter', '\\nFrom: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder \\\n",
    "            .master('local') \\\n",
    "            .appName('Altigran BD2') \\\n",
    "            .getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "NEWSGROUPS_PATH = './20-newsgroups/'\n",
    "files = [f for f in listdir(NEWSGROUPS_PATH) if isfile(join(NEWSGROUPS_PATH, f))]\n",
    "files.remove('list.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load DFs for each file in the directory and concat to a single DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "field = [StructField('class',StringType(), True),StructField('text', StringType(), True)]\n",
    "schema = StructType(field)\n",
    "union_df = spark.createDataFrame(sc.emptyRDD(), schema)\n",
    "\n",
    "\n",
    "for file in files:\n",
    "    filePath = NEWSGROUPS_PATH + file\n",
    "    newsgroup = file.split('.txt')[0]\n",
    "    df = sc.textFile(filePath).map(lambda x: (newsgroup, x)).toDF(['class', 'text'])\n",
    "    union_df = union_df.union(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanitize text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col, lower, regexp_replace\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text\n",
    "df_clean = union_df.select('class', (lower(regexp_replace('text', \"[^a-zA-Z\\\\s]\", \"\")).alias('cleaned_text')))\n",
    "\n",
    "# Tokenize text\n",
    "tokenizer = Tokenizer(inputCol='cleaned_text', outputCol='words_token')\n",
    "df_words_token = tokenizer.transform(df_clean).select('class', 'words_token')\n",
    "\n",
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol='words_token', outputCol='words_clean')\n",
    "df_words_no_stopw = remover.transform(df_words_token).select('class', 'words_clean')\n",
    "\n",
    "# Stem text\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "stemmer_udf = udf(lambda tokens: [stemmer.stem(token) for token in tokens], ArrayType(StringType()))\n",
    "df_stemmed = df_words_no_stopw.withColumn(\"words_stemmed\", stemmer_udf(\"words_clean\")).select('class', 'words_stemmed')\n",
    "\n",
    "# Filter length word >= 2\n",
    "filter_length_udf = udf(lambda row: [x for x in row if len(x) >= 2], ArrayType(StringType()))\n",
    "df_final_words = df_stemmed.withColumn('words', filter_length_udf(col('words_stemmed'))).select('class','words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(class=u'comp.sys.mac.hardware', words=[u'eapuorionoacuciedu', u'wayn', u'chen', u'subject', u're', u'disappoint', u'la', u'cie', u'articl', u'bcfdnewsserviceuciedu', u'wayn', u'chen', u'eapuorionoacuciedu', u'write', u'industri', u'sound', u'unfair', u'someon', u'oop', u'meant', u'fair', u'unfair', u'newsgroup', u'compsysmachardwar', u'documentid'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_words.take(10000)[9999]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dictinary with words index for sparce vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128250"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def emitWords(doc):\n",
    "    array = []\n",
    "    for word in doc[0]:\n",
    "        array.append( (word,1) )\n",
    "    return array\n",
    "rdd = df_final_words.select('words').rdd.flatMap(emitWords) \\\n",
    "                                         .reduceByKey(lambda a,b: a+b) \\\n",
    "                                         .sortByKey() \n",
    "\n",
    "distinct_words_array = rdd.collect()\n",
    "DICTIONARY = {}\n",
    "index = 0\n",
    "for word in distinct_words_array:\n",
    "    DICTIONARY[word[0]] = index\n",
    "    index = index + 1\n",
    "    \n",
    "NUMBER_OF_WORDS = len(DICTIONARY)\n",
    "NUMBER_OF_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dense vector representation of the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(class=u'talk.politics.guns', features=SparseVector(128250, {26073: 1.0, 82178: 1.0, 111218: 1.0}))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "def createVector(words):\n",
    "    word_indexes = sorted(set([DICTIONARY[word] for word in words]))\n",
    "    array = [1.0] * len(word_indexes)\n",
    "    return Vectors.sparse(NUMBER_OF_WORDS, word_indexes, array )\n",
    "    \n",
    "udf_func = udf(createVector,  VectorUDT())\n",
    "    \n",
    "df_final = df_final_words.withColumn('features', udf_func(col('words'))).select('class','features')\n",
    "\n",
    "df_final.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_df, test_df) = df_final.randomSplit([0.9,0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elements: 39298\n",
      "Count train elements: 35325\n",
      "Count test elements: 3973\n"
     ]
    }
   ],
   "source": [
    "print('Total elements: ' + str(df_final.count()))\n",
    "print('Count train elements: ' + str(train_df.count()))\n",
    "print('Count test elements: ' + str(test_df.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use LSH and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+--------------------+\n",
      "|             class|            features|              hashes|\n",
      "+------------------+--------------------+--------------------+\n",
      "|talk.politics.guns|(128250,[0,62,262...|[[9133272.0], [16...|\n",
      "|talk.politics.guns|(128250,[0,62,262...|[[9133272.0], [16...|\n",
      "|talk.politics.guns|(128250,[0,384,49...|[[1919832.0], [16...|\n",
      "|talk.politics.guns|(128250,[0,384,49...|[[1919832.0], [16...|\n",
      "|talk.politics.guns|(128250,[1,127,30...|[[5759166.0], [32...|\n",
      "|talk.politics.guns|(128250,[1,127,30...|[[5759166.0], [32...|\n",
      "|talk.politics.guns|(128250,[12,1199,...|[[6728792.0], [48...|\n",
      "|talk.politics.guns|(128250,[12,1199,...|[[6728792.0], [48...|\n",
      "|talk.politics.guns|(128250,[12,1199,...|[[6728792.0], [48...|\n",
      "|talk.politics.guns|(128250,[12,1199,...|[[6728792.0], [48...|\n",
      "|talk.politics.guns|(128250,[20,1195,...|[[4.8711272E7], [...|\n",
      "|talk.politics.guns|(128250,[20,1195,...|[[4.8711272E7], [...|\n",
      "|talk.politics.guns|(128250,[116,262,...|[[3509762.0], [1....|\n",
      "|talk.politics.guns|(128250,[116,262,...|[[3509762.0], [1....|\n",
      "|talk.politics.guns|(128250,[116,277,...|[[6728792.0], [20...|\n",
      "|talk.politics.guns|(128250,[116,277,...|[[6728792.0], [20...|\n",
      "|talk.politics.guns|(128250,[116,277,...|[[6728792.0], [20...|\n",
      "|talk.politics.guns|(128250,[116,374,...|[[1.6909464E7], [...|\n",
      "|talk.politics.guns|(128250,[116,374,...|[[1.6909464E7], [...|\n",
      "|talk.politics.guns|(128250,[116,402,...|[[6728792.0], [38...|\n",
      "+------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinHashLSH\n",
    "\n",
    "mh = MinHashLSH(inputCol='features', outputCol='hashes', numHashTables=5)\n",
    "model = mh.fit(train_df)\n",
    "model.transform(train_df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with regular strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def convertStringToSparseVector(string):\n",
    "    words = string.split(' ')\n",
    "    word_indexes = sorted(set([DICTIONARY[word] for word in words]))\n",
    "    array = [1.0] * len(word_indexes)\n",
    "    return Vectors.sparse(NUMBER_OF_WORDS-2, word_indexes, array )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+--------------------+------------------+\n",
      "|             class|            features|              hashes|           distCol|\n",
      "+------------------+--------------------+--------------------+------------------+\n",
      "|talk.politics.guns|(128250,[3327,743...|[[4.397985E7], [1...|0.9130434782608696|\n",
      "|talk.politics.guns|(128250,[3327,743...|[[4.397985E7], [1...|0.9130434782608696|\n",
      "|talk.politics.misc|(128250,[4036,109...|[[2.57833503E8], ...|0.9285714285714286|\n",
      "|talk.politics.misc|(128250,[4036,109...|[[2.57833503E8], ...|0.9285714285714286|\n",
      "|   sci.electronics|(128250,[1433,260...|[[4.56319426E8], ...|0.9333333333333333|\n",
      "+------------------+--------------------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "STRING = 'like kill handgun'\n",
    "\n",
    "element = convertStringToSparseVector(STRING)\n",
    "model.approxNearestNeighbors(train_df, element, 5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with regular strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+--------------------+------------------+\n",
      "|             class|            features|              hashes|           distCol|\n",
      "+------------------+--------------------+--------------------+------------------+\n",
      "|talk.religion.misc|(128250,[4455,602...|[[2.1941446E7], [...|            0.9375|\n",
      "|talk.religion.misc|(128250,[4455,602...|[[2.1941446E7], [...|            0.9375|\n",
      "|talk.religion.misc|(128250,[4455,104...|[[1.68356544E8], ...|0.9444444444444444|\n",
      "|talk.religion.misc|(128250,[4455,104...|[[1.68356544E8], ...|0.9444444444444444|\n",
      "|talk.religion.misc|(128250,[1396,445...|[[3.221884E7], [8...|0.9473684210526316|\n",
      "+------------------+--------------------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "STRING = 'god'\n",
    "\n",
    "element = convertStringToSparseVector(STRING)\n",
    "model.approxNearestNeighbors(train_df, element, 5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+------------------+\n",
      "|               class|            features|              hashes|           distCol|\n",
      "+--------------------+--------------------+--------------------+------------------+\n",
      "|comp.sys.mac.hard...|(128250,[12878,15...|[[4.4425894E7], [...|0.9444444444444444|\n",
      "|comp.sys.mac.hard...|(128250,[12878,15...|[[4.4425894E7], [...|0.9444444444444444|\n",
      "|       comp.graphics|(128250,[503,1637...|[[1.44771775E8], ...|              0.95|\n",
      "|       comp.graphics|(128250,[503,1637...|[[1.44771775E8], ...|              0.95|\n",
      "|     sci.electronics|(128250,[2354,260...|[[1.43278567E8], ...|0.9545454545454546|\n",
      "+--------------------+--------------------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "STRING = 'hardwar'\n",
    "\n",
    "element = convertStringToSparseVector(STRING)\n",
    "model.approxNearestNeighbors(train_df, element, 5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "STRING = 'computer'\n",
    "\n",
    "element = convertStringToSparseVector(STRING)\n",
    "rr = model.approxNearestNeighbors(train_df, element, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = df_final.collect()\n",
    "classified = []\n",
    "result = []\n",
    "for element in training_set:\n",
    "    result.append(model.approxNearestNeighbors(train_df, element[1], 1).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
